# -*- coding: utf-8 -*-
"""DSA Exit Exam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pIGGPRZZytPEvHflqZxYpHsik6RUeXdA

https://www.analyticsvidhya.com/datahack/contest/black-friday/
"""

import os
import zipfile

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

train_zip = os.path.join(".", "train-corrected.zip")
extract_dir = os.path.join(".", "train_extracted")

zip_path = "/mnt/data/train-corrected.zip"
extract_dir = "/mnt/data/train_extracted"

# unzip if not already extracted
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir, exist_ok=True)
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extract_dir)

# check extracted files
print("Extracted files:", os.listdir(extract_dir))

# ✅ Use the correct extracted CSV filenames
train = pd.read_csv(os.path.join(extract_dir, "/content/train-corrected.zip"))
test = pd.read_csv(os.path.join(extract_dir, "/content/test_Vges7qu.csv"))
sample_sub = pd.read_csv(os.path.join(extract_dir, "/content/test_Vges7qu.csv"))

print("Train:", train.shape, " Test:", test.shape, " Sample:", sample_sub.shape)

#Data cleaning
for col in ["User_ID","Product_ID","Gender","Age","City_Category","Stay_In_Current_City_Years"]:
    train[col] = train[col].astype("category")
    test[col] = test[col].astype("category")

for col in ["Product_Category_1","Product_Category_2","Product_Category_3"]:
    train[col] = pd.to_numeric(train[col], errors="coerce")
    test[col] = pd.to_numeric(test[col], errors="coerce")

# Simple derived feature: number of product categories present
train[["Product_Category_2","Product_Category_3"]] = train[["Product_Category_2","Product_Category_3"]].fillna(0)
test[["Product_Category_2","Product_Category_3"]] = test[["Product_Category_2","Product_Category_3"]].fillna(0)

print(train.isnull().sum())

# Drop "Comb" column from test (not needed)
if "Comb" in test.columns:
    test = test.drop(columns=["Comb"])

drop_ids = ["User_ID", "Product_ID"]
X = X.drop(columns=drop_ids)
test = test.drop(columns=drop_ids)
cat_cols = [c for c in X.columns if X[c].dtype=="object"]
num_cols = [c for c in X.columns if c not in cat_cols]

# Preprocessing + Model
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor

# Choose best model available
Model = None
try:
    from lightgbm import LGBMRegressor
    Model = LGBMRegressor(
        n_estimators=1000,
        learning_rate=0.03,
        subsample=0.9,
        colsample_bytree=0.9,
        random_state=42,
        n_jobs=-1
    )
    print("Using LightGBM")
except:
    try:
        from xgboost import XGBRegressor
        Model = XGBRegressor(
            n_estimators=1000,
            learning_rate=0.03,
            subsample=0.9,
            colsample_bytree=0.9,
            max_depth=8,
            random_state=42,
            n_jobs=-1,
            tree_method="hist"
        )
        print("Using XGBoost")
    except:
        Model = RandomForestRegressor(
            n_estimators=400,
            n_jobs=-1,
            random_state=42
        )
        print("Using RandomForest")

# Transformers
num_tf = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median"))
])
cat_tf = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("oh", OneHotEncoder(handle_unknown="ignore"))
])

preproc = ColumnTransformer(
    transformers=[
        ("num", num_tf, num_cols),
        ("cat", cat_tf, cat_cols)
    ]
)

pipe = Pipeline(steps=[
    ("prep", preproc),
    ("model", Model)
])

# -------------------------
# 1. Feature engineering
# -------------------------
# Create Num_Product_Cats in train & test
for df in [train, test]:
    df["Num_Product_Cats"] = (
        df[["Product_Category_1", "Product_Category_2", "Product_Category_3"]]
        .gt(0)
        .sum(axis=1)
    )

# -------------------------
# 2. Train preparation
# -------------------------
X_train = train.drop(["User_ID", "Product_ID", "Purchase"], axis=1, errors="ignore")
y_train = train["Purchase"]

# -------------------------
# 3. Fit pipeline
# -------------------------
pipe.fit(X_train, y_train)

# -------------------------
# 4. Test preparation
# -------------------------
X_test = test.drop(["User_ID", "Product_ID"], axis=1, errors="ignore")

# -------------------------
# 5. Predict
# -------------------------
preds = pipe.predict(X_test)

# Clip negatives (just in case)
preds = np.clip(preds, 0, None)

# -------------------------
# 6. Submission
# -------------------------
submission = sample_sub.copy()
submission["Purchase"] = preds

print(test.columns.tolist())  # sanity check
submission.to_csv("submission.csv", index=False)
print("✅ submission.csv saved!", submission.shape)
print(submission.head())

# First look
print(train.head())
print(train.info())
print(train.describe())

# Missing values
print(train.isnull().sum())

# Distribution of target variable
sns.histplot(train["Purchase"], bins=50, kde=True)
plt.title("Distribution of Purchase Amount")
plt.show()

# Gender distribution
sns.countplot(x="Gender", data=train)
plt.title("Gender Distribution")
plt.show()

# Purchase vs Gender
sns.boxplot(x="Gender", y="Purchase", data=train)
plt.title("Purchase by Gender")
plt.show()

# Age vs Purchase
sns.boxplot(x="Age", y="Purchase", data=train)
plt.title("Purchase by Age Group")
plt.show()

# Correlation heatmap (numeric only)
numeric_cols = train.select_dtypes(include="number").columns
plt.figure(figsize=(5,3))
sns.heatmap(train[numeric_cols].corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# Prepare features (drop ID columns and target)
X = train.drop(["Purchase", "User_ID", "Product_ID"], axis=1, errors="ignore")
y = train["Purchase"]

# Identify categorical and numeric features based on dtypes
cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
num_cols = X.select_dtypes(include=np.number).columns.tolist()

# Preprocessing pipelines for numerical and categorical features
num_tf = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median"))
])
cat_tf = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("oh", OneHotEncoder(handle_unknown="ignore"))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ("num", num_tf, num_cols),
        ("cat", cat_tf, cat_cols)
    ],
    remainder='passthrough' # Keep other columns (if any) - though none in this case
)

# Train-test split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42)
}

trained_models = {}
for name, model in models.items():
    # Create a pipeline for each model including preprocessing
    full_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                   ('regressor', model)])

    full_pipeline.fit(X_train, y_train)
    trained_models[name] = full_pipeline

# Evaluate each model
for name, model in trained_models.items():
    preds = model.predict(X_val)
    mse = mean_squared_error(y_val, preds)
    rmse = np.sqrt(mse)   # take square root manually
    print(f"{name} RMSE: {rmse:.2f}")